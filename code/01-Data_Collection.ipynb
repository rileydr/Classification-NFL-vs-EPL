{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Trebuchet MS; font-size:2em;\">Project 3 | NB1: Data Collection</span>\n",
    "\n",
    "Riley Robertson | Reddit Classification Project | Market Research: Sports Fans in the U.S. and England"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Primary Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:46:02.780249Z",
     "start_time": "2021-05-06T03:46:02.053253Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alert Code for main API Request function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found a python module online that will notify me via Slack when my functions are complete.\n",
    "\n",
    "Source: https://github.com/huggingface/knockknock#slack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:46:03.149031Z",
     "start_time": "2021-05-06T03:46:02.789702Z"
    }
   },
   "outputs": [],
   "source": [
    "import knockknock\n",
    "kk_url = \"https://hooks.slack.com/services/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\"\n",
    "kk_channel_name = 'jupyter-notebook'\n",
    "kk_users = ['rileyrobertsond']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subreddit info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NFL** - https://www.reddit.com/r/nfl\n",
    "\n",
    "**Premier League** - https://www.reddit.com/r/PremierLeague\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:46:03.153919Z",
     "start_time": "2021-05-06T03:46:03.151379Z"
    }
   },
   "outputs": [],
   "source": [
    "nfl = 'nfl'\n",
    "epl = 'PremierLeague'\n",
    "\n",
    "subs = [nfl, epl]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Request Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:46:03.162098Z",
     "start_time": "2021-05-06T03:46:03.157031Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using Pushshift API (https://pushshift.io/api-parameters)\n",
    "# returns a list of dictionaries from chosen subreddit\n",
    "# each dictionary containing 1 reddit submission (post) or comment\n",
    "\n",
    "def get_request(dict_params, request_type='submission'):\n",
    "    if request_type == 'submission':\n",
    "        url = f'https://api.pushshift.io/reddit/{request_type}/search'\n",
    "        res = requests.get(url, dict_params)  \n",
    "        return res.json()['data']\n",
    "\n",
    "    if request_type == 'comment':\n",
    "        return 'Comment scraping development in progress'  # res.json()['data']\n",
    "\n",
    "    else:\n",
    "        return 'Enter valid request_type (submission or comment)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-28T15:40:27.280161Z",
     "start_time": "2021-04-28T15:40:27.274931Z"
    }
   },
   "source": [
    "**Submission (post) test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:46:03.676744Z",
     "start_time": "2021-05-06T03:46:03.165095Z"
    }
   },
   "outputs": [],
   "source": [
    "# parameters setup\n",
    "params = {'subreddit': epl, 'size': '1', 'is_self': True}\n",
    "\n",
    "# assignment of function return to variable\n",
    "get_req = get_request(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:46:03.699385Z",
     "start_time": "2021-05-06T03:46:03.682464Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "subbreddit: PremierLeague\n",
      "     title: When will Premier League teams figure out Thomas T...\n",
      "    author: Lersbyte\n",
      "   created: 1620259116\n",
      "  comments: 0\n",
      "       url: https://www.reddit.com/r/PremierLeague/comments/n5uajf/when_will_premier_league_teams_figure_out_thomas/\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "f'''\n",
    "subbreddit: {get_req[0]['subreddit']}\n",
    "     title: {get_req[0]['title'][:50]}...\n",
    "    author: {get_req[0]['author']}\n",
    "   created: {get_req[0]['created_utc']}\n",
    "  comments: {get_req[0]['num_comments']}\n",
    "       url: {get_req[0]['url']}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wanted to make the process of scraping as easy as possible, so I built a function that has evolved over the course of my work for OverArmor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:46:03.732027Z",
     "start_time": "2021-05-06T03:46:03.711384Z"
    },
    "code_folding": [
     21
    ]
   },
   "outputs": [],
   "source": [
    "# Function to make [n] requests of 100 posts. \n",
    "# Each request will be for a period of [win] days and the periods will not overlap\n",
    "# Exports .csv of API responses for each subreddit\n",
    "\n",
    "@knockknock.slack_sender(webhook_url=kk_url, channel=kk_channel_name, user_mentions=kk_users)\n",
    "def api_requests(n, win, subs_list, output_folder, size=100, is_self=True, is_video=False):\n",
    "\n",
    "    # For Loop to scrape multiple subreddits:\n",
    "    for sub in subs_list:\n",
    "        \n",
    "        # List instantiation for error/event logging\n",
    "        report = []\n",
    "        \n",
    "        # Onscreen display during loops:        \n",
    "        cur_datetime = dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print(f'API request initiated. Scraping r/{sub} in progress...')\n",
    "        print(f'   Current time: {cur_datetime}')\n",
    "        print('')\n",
    "        \n",
    "        # Adding updates to event log\n",
    "        report.append({'event': f'API request from r/{sub} began',\n",
    "                       'datetime': f'{cur_datetime}',\n",
    "                       'exception': ''\n",
    "                       })\n",
    "\n",
    "        # Setting parameters to use in data requests from the API. Parameters mostly set by function args. \n",
    "        params = {\n",
    "            'subreddit': sub,\n",
    "            'size': size,\n",
    "            'is_self': is_self,\n",
    "            'is_video': is_video,\n",
    "            'selftext:not': '[removed]'      # thanks to Amanda for posting this in the groupwork channel\n",
    "        }\n",
    "\n",
    "        list_data = []\n",
    "        \n",
    "        for i in range(1, n+1):               # I used the demo notebook to figure out these time\n",
    "            params['after'] = f'{i * win}d'   # parameters and loop structure using n and day_window\n",
    "\n",
    "            # Try/Except to handle any errors in the get_requests\n",
    "            try:\n",
    "                new_data = get_request(params)\n",
    "\n",
    "            # Adding errors to event log if get_request fails\n",
    "            except Exception as e:                                              \n",
    "                cur_datetime = dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                report.append({'event': f'REQUEST FAILED: {i}',\n",
    "                               'datetime': f'{cur_datetime}',\n",
    "                               'exception': f'{e}'\n",
    "                               })\n",
    "                \n",
    "                # Onscreen display - Failure of single request\n",
    "                print(f'Request failed: {i}')\n",
    "                print(f'  Current time: {cur_datetime}')\n",
    "                print('')\n",
    "                time.sleep(.25)\n",
    "                continue\n",
    "                \n",
    "            # Adding newly pulled data to list for eventual DataFrame conversion and export\n",
    "            list_data.extend(new_data)\n",
    "    \n",
    "            # Adding completion updates to event log\n",
    "            cur_datetime = dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            report.append({'event': f'Request complete: {i}',\n",
    "                           'datetime': f'{cur_datetime}',\n",
    "                           'exception': ''\n",
    "                           })\n",
    "            \n",
    "            # Onscreen updates for every 10 requests to provide timing updates\n",
    "            if i % 5 == 0:\n",
    "                print(f'Request complete: {i}')\n",
    "                print(f'    Current time: {cur_datetime}')\n",
    "                print('')\n",
    "            df_report = pd.DataFrame(report)\n",
    "            df_report.to_csv(f'../git_ignore/output/report.csv', index=False)\n",
    "            time.sleep(.25)\n",
    "\n",
    "            \n",
    "        # Setting up list of desired features to keep in primary output. Others will be excluded:\n",
    "        features = ['subreddit', 'created_utc', 'author', 'num_comments',\n",
    "                    'score', 'is_self', 'link_flair_text','title', 'selftext', 'full_link']\n",
    "            \n",
    "        # creating two DataFrames from list_data after all requests have been made and completed \n",
    "        df_output = pd.DataFrame(list_data)\n",
    "        df_outputfull = pd.DataFrame(list_data)\n",
    "        \n",
    "        # filtering the primary DataFrame\n",
    "        df_output = df_output[features]\n",
    "        \n",
    "        # creating simple date and time columns using utc code\n",
    "        # https://docs.python.org/3/library/datetime.html#date-objects\n",
    "        # https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\n",
    "        df_output['date'] = df_output['created_utc'].map(lambda x: dt.datetime.fromtimestamp(x).strftime('%Y-%m-%d'))\n",
    "        df_output['time'] = df_output['created_utc'].map(lambda x: dt.datetime.fromtimestamp(x).strftime('%H:%M:%S'))\n",
    "\n",
    "        # Exporting data with selected/filtered features\n",
    "        # using if/else statement to accomodate varied input formatting\n",
    "        df_output.reset_index(inplace=True)\n",
    "        cur_datetime = dt.datetime.now().strftime('%Y_%m_%d-%H_%M_%S')\n",
    "\n",
    "        if output_folder[-1] == '/':\n",
    "            df_output.to_csv(f'{output_folder}{cur_datetime}_data_{sub}.csv', index=False,)\n",
    "        else:\n",
    "            df_output.to_csv(f'{output_folder}/{cur_datetime}_data_{sub}.csv', index=False,)     \n",
    "            \n",
    "        # Adding last update and then exporting event log as a CSV report\n",
    "        # using if/else statement to accomodate varied input formatting\n",
    "        cur_datetime = dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        report.append({'event': f'r/{sub} data files saved to output folder', 'datetime': f'{cur_datetime}'})\n",
    "\n",
    "        df_report = pd.DataFrame(report)\n",
    "        cur_datetime = dt.datetime.now().strftime('%Y_%m_%d-%H_%M_%S')\n",
    "        \n",
    "        if output_folder[-1] == '/':\n",
    "            df_report.to_csv(f'{output_folder}{cur_datetime}_report_{sub}.csv', index=False,)\n",
    "        else:\n",
    "            df_report.to_csv(f'{output_folder}/{cur_datetime}_report_{sub}.csv', index=False,)\n",
    "\n",
    "        report = []\n",
    "\n",
    "        # Onscreen display - completion of each subreddit scrape\n",
    "        cur_datetime = dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print(f'r/{sub} data and report files saved to output folder')\n",
    "        print(f'   Current time: {cur_datetime}')\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick test with a small number of requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:46:40.028132Z",
     "start_time": "2021-05-06T03:46:03.734049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API request initiated. Scraping r/nfl in progress...\n",
      "   Current time: 2021-05-05 20:46:03\n",
      "\n",
      "Request complete: 5\n",
      "    Current time: 2021-05-05 20:46:10\n",
      "\n",
      "Request complete: 10\n",
      "    Current time: 2021-05-05 20:46:16\n",
      "\n",
      "Request complete: 15\n",
      "    Current time: 2021-05-05 20:46:21\n",
      "\n",
      "r/nfl data and report files saved to output folder\n",
      "   Current time: 2021-05-05 20:46:22\n",
      "\n",
      "API request initiated. Scraping r/PremierLeague in progress...\n",
      "   Current time: 2021-05-05 20:46:22\n",
      "\n",
      "Request complete: 5\n",
      "    Current time: 2021-05-05 20:46:27\n",
      "\n",
      "Request complete: 10\n",
      "    Current time: 2021-05-05 20:46:33\n",
      "\n",
      "Request complete: 15\n",
      "    Current time: 2021-05-05 20:46:39\n",
      "\n",
      "r/PremierLeague data and report files saved to output folder\n",
      "   Current time: 2021-05-05 20:46:39\n",
      "\n"
     ]
    }
   ],
   "source": [
    "api_requests(15, 2, ['nfl', 'PremierLeague'], output_folder='../data/0_from_api/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Scrapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once my function was initially complete, I tested the waters to see what I would get back. I pulled these smaller datasets into the cleaning process and realized fairly quickly that I would need more to ensure that I had a balanced sample from both subreddits. As I dropped rows based on certain criteria, the posts from r/premierleague were dropped at a much higher rate than r/nfl, so I needed to get enough Premier League content that I wouldn't have to fight to recover posts with insufficient or unusable date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:46:40.034041Z",
     "start_time": "2021-05-06T03:46:40.030289Z"
    }
   },
   "outputs": [],
   "source": [
    "# v1\n",
    "# api_requests(200, 2, ['nfl', 'PremierLeague'], output_folder='../git_ignore/output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:46:40.060605Z",
     "start_time": "2021-05-06T03:46:40.048086Z"
    }
   },
   "outputs": [],
   "source": [
    "# v2\n",
    "# api_requests(400, 2, ['nfl', 'PremierLeague'], output_folder='../git_ignore/output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:46:40.080756Z",
     "start_time": "2021-05-06T03:46:40.065696Z"
    }
   },
   "outputs": [],
   "source": [
    "# v3\n",
    "# api_requests(800, 2, ['nfl', 'PremierLeague'], output_folder='../git_ignore/output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final scrape was enough and after cleaning, I still had roughly 5000 solid posts from the Premier League subreddit which was in the range I wanted for the sake of building my model. going to retrieve 100 posts for each iteration, totaling 100,000 posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:46:40.092029Z",
     "start_time": "2021-05-06T03:46:40.086788Z"
    }
   },
   "outputs": [],
   "source": [
    "# v4\n",
    "# api_requests(1000, 2, ['nfl', 'PremierLeague'], output_folder='../git_ignore/output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another advantage of pulling more data than necessary, is that I didn't have to come back to pull more data again. In addition, the raw data can continue to serve as a source of information for OverArmor as they continue to work on their marketing strategies.\n",
    "\n",
    "Once I clean and prepare this for EDA and Modeling, I'll have fulfilled OverArmor's first request. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "262px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
