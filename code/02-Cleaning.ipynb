{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Trebuchet MS; font-size:2em;\">Project 3 | NB2: Cleaning and Preprocessing</span>\n",
    "\n",
    "Riley Robertson | Reddit Classification Project | Market Research: Sports Fans in the U.S. and England"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Imports and setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I began my process by importing basic libraries and as I cleaned, I returned to add modules as necessary. I also set preferences, assigned variables, imported my data, and set up my main dataframe so I could begin cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:16.503849Z",
     "start_time": "2021-05-06T03:29:13.881679Z"
    }
   },
   "outputs": [],
   "source": [
    "# basics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "\n",
    "# custom\n",
    "import utilities.densmore as dns\n",
    "\n",
    "# date and time \n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "# for CVEC test\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:18.307070Z",
     "start_time": "2021-05-06T03:29:16.508323Z"
    }
   },
   "outputs": [],
   "source": [
    "df_nfl = pd.read_csv('../data/1_raw/raw_nfl_v4.csv', low_memory=False)\n",
    "df_epl = pd.read_csv('../data/1_raw/raw_epl_v4.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:18.319815Z",
     "start_time": "2021-05-06T03:29:18.311669Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((99661, 13), (99589, 13))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nfl.shape, df_epl.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:18.397186Z",
     "start_time": "2021-05-06T03:29:18.325071Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df_epl, df_nfl], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:18.533799Z",
     "start_time": "2021-05-06T03:29:18.400458Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 199250 entries, 0 to 199249\n",
      "Data columns (total 13 columns):\n",
      " #   Column           Non-Null Count   Dtype \n",
      "---  ------           --------------   ----- \n",
      " 0   index            199250 non-null  int64 \n",
      " 1   subreddit        199250 non-null  object\n",
      " 2   created_utc      199250 non-null  int64 \n",
      " 3   author           199250 non-null  object\n",
      " 4   num_comments     199250 non-null  int64 \n",
      " 5   score            199250 non-null  int64 \n",
      " 6   is_self          199250 non-null  bool  \n",
      " 7   link_flair_text  29684 non-null   object\n",
      " 8   title            199250 non-null  object\n",
      " 9   selftext         168312 non-null  object\n",
      " 10  full_link        199250 non-null  object\n",
      " 11  date             199250 non-null  object\n",
      " 12  time             199250 non-null  object\n",
      "dtypes: bool(1), int64(4), object(8)\n",
      "memory usage: 18.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# df.shape\n",
    "# df.head()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming PremierLeague Subreddit to 'epl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:18.553497Z",
     "start_time": "2021-05-06T03:29:18.535460Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nfl              99661\n",
       "PremierLeague    99589\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:18.588287Z",
     "start_time": "2021-05-06T03:29:18.555406Z"
    }
   },
   "outputs": [],
   "source": [
    "df['subreddit'] = df['subreddit'].map(lambda x: 'epl' if x == 'PremierLeague' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:18.621501Z",
     "start_time": "2021-05-06T03:29:18.596965Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nfl    99661\n",
       "epl    99589\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Sorting and Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I cleaned, I realized that there were some columns that I didn't ultimately need, so I filtered out some of the columns that I initially included in my scraped data and resorted the remaining columns for ease of viewing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:18.652433Z",
     "start_time": "2021-05-06T03:29:18.627076Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df[['subreddit', 'created_utc', 'date', 'time', 'link_flair_text', 'author', 'score', 'num_comments', 'index',  'title', 'selftext']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:18.660801Z",
     "start_time": "2021-05-06T03:29:18.655114Z"
    }
   },
   "outputs": [],
   "source": [
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:18.698832Z",
     "start_time": "2021-05-06T03:29:18.666146Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nfl    99661\n",
       "epl    99589\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown by the value counts above, I'm starting out with about 100,000 posts for each subreddit. I originally started with fewer, but after I began cleaning, I was quickly running of posts that that had the conditions I wanted. I returned to my Data Collection notebook and increased the number of posts to request from the API so that I'd begin my cleaning with a much greater volume of posts than I would eventually need. That way, I could be more decisive in dropping rows rather than trying to salvage content from posts that had incomplete or low quality information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Basic Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nulls only exist in two columns: `link_flair_text` and `selftext`. \n",
    "\n",
    "I knew I had enough data that I could drop all of the posts with empty `selftext` fields, but I didn't want to lose the posts without tags (there are many). So I put 'none' into the `link_flair_text` fields and removed all rows with nulls after that, which left about 80,000 posts per subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:18.768963Z",
     "start_time": "2021-05-06T03:29:18.700900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 199250 entries, 0 to 199249\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count   Dtype \n",
      "---  ------           --------------   ----- \n",
      " 0   subreddit        199250 non-null  object\n",
      " 1   created_utc      199250 non-null  int64 \n",
      " 2   date             199250 non-null  object\n",
      " 3   time             199250 non-null  object\n",
      " 4   link_flair_text  29684 non-null   object\n",
      " 5   author           199250 non-null  object\n",
      " 6   score            199250 non-null  int64 \n",
      " 7   num_comments     199250 non-null  int64 \n",
      " 8   index            199250 non-null  int64 \n",
      " 9   title            199250 non-null  object\n",
      " 10  selftext         168312 non-null  object\n",
      "dtypes: int64(4), object(7)\n",
      "memory usage: 16.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:18.948195Z",
     "start_time": "2021-05-06T03:29:18.771565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 168312 entries, 0 to 199248\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count   Dtype \n",
      "---  ------           --------------   ----- \n",
      " 0   subreddit        168312 non-null  object\n",
      " 1   created_utc      168312 non-null  int64 \n",
      " 2   date             168312 non-null  object\n",
      " 3   time             168312 non-null  object\n",
      " 4   link_flair_text  168312 non-null  object\n",
      " 5   author           168312 non-null  object\n",
      " 6   score            168312 non-null  int64 \n",
      " 7   num_comments     168312 non-null  int64 \n",
      " 8   index            168312 non-null  int64 \n",
      " 9   title            168312 non-null  object\n",
      " 10  selftext         168312 non-null  object\n",
      "dtypes: int64(4), object(7)\n",
      "memory usage: 15.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# fill nulls in link_flair_text column\n",
    "df['link_flair_text'].fillna('none', inplace=True)\n",
    "\n",
    "# remove all rows with nulls in selftext column\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:18.973773Z",
     "start_time": "2021-05-06T03:29:18.951657Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "epl    86351\n",
       "nfl    81961\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping duplicates brings down PremierLeague posts to a good range, but the number of NFL posts is still much greater than necessary. As I move forward, I'll work on bringing down the number of NFL posts to at least roughly match that of the PremierLeague posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:19.049242Z",
     "start_time": "2021-05-06T03:29:18.976903Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59730, 11)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates(subset=['title'], inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:19.067213Z",
     "start_time": "2021-05-06T03:29:19.051726Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nfl    53173\n",
       "epl     6557\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:19.378271Z",
     "start_time": "2021-05-06T03:29:19.070237Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56178, 11)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates(subset=['selftext'], inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:19.391700Z",
     "start_time": "2021-05-06T03:29:19.381645Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nfl    49832\n",
       "epl     6346\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posts with deleted body text (selftext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:19.451872Z",
     "start_time": "2021-05-06T03:29:19.394472Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nfl    49831\n",
       "epl     6324\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(axis=0, \n",
    "        labels=df[df['selftext'].str.startswith('[deleted]')].index, # Submissions with deleted selftext\n",
    "        inplace=True)\n",
    "\n",
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posts with Markdown tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:19.518733Z",
     "start_time": "2021-05-06T03:29:19.453874Z"
    }
   },
   "outputs": [],
   "source": [
    "markdowns = df[df['selftext'].str.contains('\\|')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:19.526294Z",
     "start_time": "2021-05-06T03:29:19.520716Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nfl    3906\n",
       "epl     201\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markdowns['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:19.543930Z",
     "start_time": "2021-05-06T03:29:19.528210Z"
    }
   },
   "outputs": [],
   "source": [
    "df.drop(axis=0, labels=markdowns.index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Deep Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeated Post Titles (Complex Duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:19.552722Z",
     "start_time": "2021-05-06T03:29:19.550775Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df['selftext'].value_counts()[:30]\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at the value counts for the 'selftext' field, I realized that there were a lot of posts that the same, or very slightly different body text. But they weren't caught by the code to remove duplicates above. So I used the following code to look at the top 20 most common titles and there were quite a few that were re-used many times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:19.569450Z",
     "start_time": "2021-05-06T03:29:19.561671Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# df['title'].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Readable results from above code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Post Title                                         | Count | ┃ | Post Title                          | Count | ┃ | Post Title                                   | Count |\n",
    "|:---------------------------------------------------|:------|:-:|:------------------------------------|:------|:-:|:---------------------------------------------|:------|\n",
    "| Shitpost Saturday                                  | 174   | ┃ | Talko Tuesday                       | 124   | ┃ | r/PremierLeague Midweek Musings              | 13    |\n",
    "| Water Cooler Wednesday                             | 159   | ┃ | r/PremierLeague Daily Discussion    | 71    | ┃ | Weekly /r/PremierLeague Subreddit Suggestion | 11    |\n",
    "| Free Talk Friday                                   | 158   | ┃ | This Week's Top /r/NFL [Highlight]s | 22    | ┃ | Test                                         | 11    |\n",
    "| Sunday Brunch                                      | 157   | ┃ | Weekend Wrap Up                     | 21    | ┃ | Daily Open Discussion Thread                 | 11    |\n",
    "| Thursday Talk Thread... Yes That's The Thread Name | 141   | ┃ | Question                            | 15    | ┃ | Weekly Transfer Discussion Thread            | 8     |\n",
    "| Weekend Wrapup                                     | 130   | ┃ | NFL Power Rankings (Combined)       | 15    | ┃ | test                                         | 7     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They fell into several categories:\n",
    "1. Open threads meant for discussion of topics of any kind, even if unrelated to the topic of the subreddit.\n",
    "2. Discussion threads in which the topics might be related, but all of the content is in the comments rather than the body of the post\n",
    "3. Posts with code and/or little-to-no useful content\n",
    "4. Commonly used titles by different users to introduce a topic-relevant post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:19.583799Z",
     "start_time": "2021-05-06T03:29:19.578271Z"
    }
   },
   "outputs": [],
   "source": [
    "repeat_titles = [\"Shitpost Saturday\", \"Water Cooler Wednesday\", \"Free Talk Friday\", \"Sunday Brunch\", \n",
    "                 \"Thursday Talk Thread... Yes That's The Thread Name\", \"Weekend Wrapup\", \"Talko Tuesday\",\n",
    "                 \"r/PremierLeague Daily Discussion\", \"This Week's Top /r/NFL [Highlight]s\", \n",
    "                 \"Weekend Wrap Up\", \"NFL Power Rankings (Combined)\", \"r/PremierLeague Midweek Musings\", \n",
    "                 \"Whose Line is it Anyways Wednesday--Offseason Edition\", \n",
    "                 \"Weekly /r/PremierLeague Subreddit Suggestion\", \"Test\", \"Daily Open Discussion Thread\",\n",
    "                 \"Weekly Transfer Discussion Thread\", \"test\", \"Your Weekly /r/nfl Recap\", \n",
    "                 \"NFL Power Rankings (Combined) Week 0\",\n",
    "                 \"Should Ole stay at Manchester United or not? If he got sacked by the board, who will be the best replacement. Comment your thoughts below\"\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.062025Z",
     "start_time": "2021-05-06T03:29:19.586701Z"
    }
   },
   "outputs": [],
   "source": [
    "for title in repeat_titles:\n",
    "    title_df = df[df['title'] == title]  \n",
    "    df.drop(axis=0, labels=title_df.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.084745Z",
     "start_time": "2021-05-06T03:29:20.068963Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nfl    45921\n",
       "epl     6117\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PremierLeague Poll Posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found 91 posts from the PremierLeague subreddit that contained a lot of unnecessary information and formatting was such that vectorizing would be significantly more complicated. I decided to simply remove them for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.101513Z",
     "start_time": "2021-05-06T03:29:20.087604Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('[View Poll](https://www.reddit.com/poll/g437k5)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.146574Z",
     "start_time": "2021-05-06T03:29:20.103568Z"
    }
   },
   "outputs": [],
   "source": [
    "poll_posts = df[df['selftext'].str.startswith('  [View Poll]')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.183879Z",
     "start_time": "2021-05-06T03:29:20.151625Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 11)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poll_posts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.215168Z",
     "start_time": "2021-05-06T03:29:20.186325Z"
    }
   },
   "outputs": [],
   "source": [
    "df.drop(axis=0, labels=poll_posts.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.234875Z",
     "start_time": "2021-05-06T03:29:20.217676Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nfl    45921\n",
       "epl     6117\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PremierLeague Match Threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found 91 posts from the PremierLeague subreddit that contained a lot of unnecessary information and formatting was such that vectorizing would be significantly more complicated. I decided to simply remove them for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.244227Z",
     "start_time": "2021-05-06T03:29:20.238235Z"
    }
   },
   "outputs": [],
   "source": [
    "match_thread_titles = ('[Match Thread]', \n",
    "                       '[Match thread]', \n",
    "                       '[match Thread]', \n",
    "                       '[match thread]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.292328Z",
     "start_time": "2021-05-06T03:29:20.252415Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    52029\n",
       "True         9\n",
       "Name: title, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'].str.startswith(match_thread_titles).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.334234Z",
     "start_time": "2021-05-06T03:29:20.297797Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    52029\n",
       "True         9\n",
       "Name: title, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'].str.startswith(match_thread_titles).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.368216Z",
     "start_time": "2021-05-06T03:29:20.336135Z"
    }
   },
   "outputs": [],
   "source": [
    "match_threads = df[df['title'].str.startswith(match_thread_titles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.385288Z",
     "start_time": "2021-05-06T03:29:20.371269Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 11)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_threads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.408981Z",
     "start_time": "2021-05-06T03:29:20.388864Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nfl    45921\n",
       "epl     6117\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing NFL posts with Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Game Thread, Serious, Look Here!, and others that are mostly comment threads of unrelated topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.426847Z",
     "start_time": "2021-05-06T03:29:20.412856Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Look Here!                        1805\n",
       "Game Thread                       1061\n",
       "Serious                            748\n",
       "Free Talk                          492\n",
       "Free talk                          437\n",
       "Removed: Rule 2 - Invalid Post     116\n",
       "Post Game Thread                    72\n",
       "Trash Talk                          60\n",
       "Look Here                           51\n",
       "game                                42\n",
       "Name: link_flair_text, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nfl['link_flair_text'].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.452832Z",
     "start_time": "2021-05-06T03:29:20.433124Z"
    }
   },
   "outputs": [],
   "source": [
    "nfl_with_tags = df[(df['link_flair_text'] != 'none') & (df['subreddit'] == 'nfl')]\n",
    "df.drop(axis=0, labels=nfl_with_tags.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.473914Z",
     "start_time": "2021-05-06T03:29:20.457269Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "none                       47228\n",
       "Discussion                  1583\n",
       "Question                     919\n",
       "Poll                         564\n",
       ":xpl: Premier League         246\n",
       "News                          79\n",
       ":mun: Manchester United       72\n",
       ":liv: Liverpool               62\n",
       ":ars: Arsenal                 57\n",
       ":che: Chelsea                 53\n",
       "Name: link_flair_text, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['link_flair_text'].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.508276Z",
     "start_time": "2021-05-06T03:29:20.477273Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nfl    45124\n",
       "epl     5553\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(axis=0, labels=df[(df['link_flair_text'] == 'Poll')].index, inplace=True)\n",
    "\n",
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NFL Posts Filtered by String Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to reduce the number of posts I had from the r/nfl, I decided to filter based on length.\n",
    "\n",
    "First, I created a DataFrame that contained only nfl posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.526203Z",
     "start_time": "2021-05-06T03:29:20.512523Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45124, 11)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered = df[df['subreddit'] == 'nfl']\n",
    "\n",
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then created a second DataFrame that contained only the rows I want to keep (rows with post lengths between 500 and 1200 characters was where I landed after several tests until I got the count of NFL posts down to a similar number as that of the EPL posts.\n",
    "\n",
    "Taking this slightly roundabout way allowed me to see the number of posts I'd have remaining once I removed the excess from the main DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.592392Z",
     "start_time": "2021-05-06T03:29:20.527898Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7245, 11)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lengthlimits = df[(df['selftext'].str.len()>500) & \\\n",
    "                     (df['selftext'].str.len()<1200) & \\\n",
    "                     (df['subreddit'] == 'nfl')]\n",
    "df_lengthlimits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the index of that second DataFrame, I removed all the posts I want to keep from the DataFrame I created above: 'df_filtered', thus giving me a DataFrame containing all of the posts I want to exclude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.606695Z",
     "start_time": "2021-05-06T03:29:20.594119Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_filtered = df_filtered.drop(axis=0, labels=df_lengthlimits.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.611938Z",
     "start_time": "2021-05-06T03:29:20.608369Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37879, 11)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that filtered DataFrame, I was able to use its index to drop all of the unwanted posts from the primary DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.627581Z",
     "start_time": "2021-05-06T03:29:20.613668Z"
    }
   },
   "outputs": [],
   "source": [
    "df.drop(axis=0, labels=df_filtered.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.635842Z",
     "start_time": "2021-05-06T03:29:20.629743Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nfl    7245\n",
       "epl    5553\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on, I realized that there some extremely long posts (upwards of 25,000-30,000 characters) on the epl page that really skewed my distributions in the EDA section. I came back to remove those outliers and then move forward again from here.\n",
    "\n",
    "The content is valuable, though, so I didn't want to trim too much. rather than trimming as far as a max of 1200 characters like I did for the NFL posts, I cut it off at 3,000. The distribution will still be off, but not nearly to the severe degree it was before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.651040Z",
     "start_time": "2021-05-06T03:29:20.637714Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127, 11)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['subreddit'] == 'epl') & (df['selftext'].str.len()>3000)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.664161Z",
     "start_time": "2021-05-06T03:29:20.652760Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df[df['selftext'].str.len()<3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.669427Z",
     "start_time": "2021-05-06T03:29:20.666063Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12671, 11)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.677522Z",
     "start_time": "2021-05-06T03:29:20.671599Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nfl    7245\n",
       "epl    5426\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing so many rows, the DataFrame's index had gaps in its sequencing, so I decided to reset it to clean it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.696279Z",
     "start_time": "2021-05-06T03:29:20.679442Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>epl</td>\n",
       "      <td>1619271770</td>\n",
       "      <td>2021-04-24</td>\n",
       "      <td>06:42:50</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>CC-33</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>My thoughts and prayers are with Jurgen Klopp ...</td>\n",
       "      <td>Imagine being Jurgen Klopp right now, arguably...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>epl</td>\n",
       "      <td>1619278607</td>\n",
       "      <td>2021-04-24</td>\n",
       "      <td>08:36:47</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>Cheerful_Jerry9603</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>Premier League Players who should finish off t...</td>\n",
       "      <td>Chinese Super League is known to be the last p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>epl</td>\n",
       "      <td>1619283368</td>\n",
       "      <td>2021-04-24</td>\n",
       "      <td>09:56:08</td>\n",
       "      <td>Question</td>\n",
       "      <td>alphaftw1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>Hypothetical situation, what happens if both c...</td>\n",
       "      <td>So let’s say this season, arsenal win the euro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>epl</td>\n",
       "      <td>1619283692</td>\n",
       "      <td>2021-04-24</td>\n",
       "      <td>10:01:32</td>\n",
       "      <td>Question</td>\n",
       "      <td>imjonathvn</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>15</td>\n",
       "      <td>Norwich, Watford, and Bournemouth might all ge...</td>\n",
       "      <td>Norwich and Watford have already been promoted...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit  created_utc        date      time link_flair_text  \\\n",
       "7        epl   1619271770  2021-04-24  06:42:50      Discussion   \n",
       "9        epl   1619278607  2021-04-24  08:36:47      Discussion   \n",
       "14       epl   1619283368  2021-04-24  09:56:08        Question   \n",
       "15       epl   1619283692  2021-04-24  10:01:32        Question   \n",
       "\n",
       "                author  score  num_comments  index  \\\n",
       "7                CC-33      1             3      7   \n",
       "9   Cheerful_Jerry9603      1             2      9   \n",
       "14           alphaftw1      1             8     14   \n",
       "15          imjonathvn      1            24     15   \n",
       "\n",
       "                                                title  \\\n",
       "7   My thoughts and prayers are with Jurgen Klopp ...   \n",
       "9   Premier League Players who should finish off t...   \n",
       "14  Hypothetical situation, what happens if both c...   \n",
       "15  Norwich, Watford, and Bournemouth might all ge...   \n",
       "\n",
       "                                             selftext  \n",
       "7   Imagine being Jurgen Klopp right now, arguably...  \n",
       "9   Chinese Super League is known to be the last p...  \n",
       "14  So let’s say this season, arsenal win the euro...  \n",
       "15  Norwich and Watford have already been promoted...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[5:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'index' column can serve as a record of each posts original index number in case it's ever needed going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.705055Z",
     "start_time": "2021-05-06T03:29:20.700362Z"
    }
   },
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.721645Z",
     "start_time": "2021-05-06T03:29:20.708418Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>epl</td>\n",
       "      <td>1619271770</td>\n",
       "      <td>2021-04-24</td>\n",
       "      <td>06:42:50</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>CC-33</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>My thoughts and prayers are with Jurgen Klopp ...</td>\n",
       "      <td>Imagine being Jurgen Klopp right now, arguably...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>epl</td>\n",
       "      <td>1619278607</td>\n",
       "      <td>2021-04-24</td>\n",
       "      <td>08:36:47</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>Cheerful_Jerry9603</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>Premier League Players who should finish off t...</td>\n",
       "      <td>Chinese Super League is known to be the last p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>epl</td>\n",
       "      <td>1619283368</td>\n",
       "      <td>2021-04-24</td>\n",
       "      <td>09:56:08</td>\n",
       "      <td>Question</td>\n",
       "      <td>alphaftw1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>Hypothetical situation, what happens if both c...</td>\n",
       "      <td>So let’s say this season, arsenal win the euro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>epl</td>\n",
       "      <td>1619283692</td>\n",
       "      <td>2021-04-24</td>\n",
       "      <td>10:01:32</td>\n",
       "      <td>Question</td>\n",
       "      <td>imjonathvn</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>15</td>\n",
       "      <td>Norwich, Watford, and Bournemouth might all ge...</td>\n",
       "      <td>Norwich and Watford have already been promoted...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit  created_utc        date      time link_flair_text  \\\n",
       "5       epl   1619271770  2021-04-24  06:42:50      Discussion   \n",
       "6       epl   1619278607  2021-04-24  08:36:47      Discussion   \n",
       "7       epl   1619283368  2021-04-24  09:56:08        Question   \n",
       "8       epl   1619283692  2021-04-24  10:01:32        Question   \n",
       "\n",
       "               author  score  num_comments  index  \\\n",
       "5               CC-33      1             3      7   \n",
       "6  Cheerful_Jerry9603      1             2      9   \n",
       "7           alphaftw1      1             8     14   \n",
       "8          imjonathvn      1            24     15   \n",
       "\n",
       "                                               title  \\\n",
       "5  My thoughts and prayers are with Jurgen Klopp ...   \n",
       "6  Premier League Players who should finish off t...   \n",
       "7  Hypothetical situation, what happens if both c...   \n",
       "8  Norwich, Watford, and Bournemouth might all ge...   \n",
       "\n",
       "                                            selftext  \n",
       "5  Imagine being Jurgen Klopp right now, arguably...  \n",
       "6  Chinese Super League is known to be the last p...  \n",
       "7  So let’s say this season, arsenal win the euro...  \n",
       "8  Norwich and Watford have already been promoted...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[5:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Column Clean-up**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming 'selftext' to 'post'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I renamed this column because it felt more intuitive and it's shorter/easier to type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.727058Z",
     "start_time": "2021-05-06T03:29:20.723815Z"
    }
   },
   "outputs": [],
   "source": [
    "df['post'] = df['selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.738680Z",
     "start_time": "2021-05-06T03:29:20.729324Z"
    }
   },
   "outputs": [],
   "source": [
    "df.drop(columns='selftext', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging 'title' and 'post' into an 'alltext' column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to merge title and post into a single column so that I could keep all relevant text that I'd be using for modeling together in a single field. And any additional changes I made to the text could be made to that column only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.758728Z",
     "start_time": "2021-05-06T03:29:20.741616Z"
    }
   },
   "outputs": [],
   "source": [
    "df['alltext'] = df['title'] + ' ' + df['post']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.762331Z",
     "start_time": "2021-05-06T03:29:20.760468Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# checks\n",
    "# pd.DataFrame(df.iloc[343]).T[['title', 'post', 'alltext']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.779965Z",
     "start_time": "2021-05-06T03:29:20.764473Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(821, 822)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pd.DataFrame(df.iloc[343]).T['title'][343]) + len(pd.DataFrame(df.iloc[343]).T['post'][343]), \\\n",
    "len(pd.DataFrame(df.iloc[343]).T['alltext'][343])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming 'num_comments' to 'comments'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shortening column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.793159Z",
     "start_time": "2021-05-06T03:29:20.781532Z"
    }
   },
   "outputs": [],
   "source": [
    "df['comments'] = df['num_comments']\n",
    "\n",
    "df.drop(columns='num_comments', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming 'link_flair_text' to 'tag'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.806557Z",
     "start_time": "2021-05-06T03:29:20.795298Z"
    }
   },
   "outputs": [],
   "source": [
    "df['tag'] = df['link_flair_text']\n",
    "\n",
    "df.drop(columns='link_flair_text', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating 'target' column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I created a column that represents each row's subreddits as a 1 or 0, which will allow our models to easily recognize and interact with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.817716Z",
     "start_time": "2021-05-06T03:29:20.808323Z"
    }
   },
   "outputs": [],
   "source": [
    "df['target'] = df['subreddit'].map(lambda x: 1 if x == 'nfl' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-ordering columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-03T02:34:07.230275Z",
     "start_time": "2021-05-03T02:34:07.225573Z"
    }
   },
   "source": [
    "Old Order:\n",
    "\n",
    "'subreddit',  \n",
    "'created_utc', 'date', 'time',  \n",
    "'link_flair_text', 'author', 'score', 'num_comments',  \n",
    "'index',  'title', 'selftext'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.826976Z",
     "start_time": "2021-05-06T03:29:20.819553Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df[[\n",
    "        'subreddit',\n",
    "         'target', \n",
    "         'author', \n",
    "         'score', \n",
    "         'comments', \n",
    "         'tag', \n",
    "         'index',\n",
    "         'created_utc', \n",
    "         'date', \n",
    "         'time',\n",
    "         'title', \n",
    "         'post', \n",
    "         'alltext'\n",
    "        ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.844805Z",
     "start_time": "2021-05-06T03:29:20.829675Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12671 entries, 0 to 12670\n",
      "Data columns (total 13 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   subreddit    12671 non-null  object\n",
      " 1   target       12671 non-null  int64 \n",
      " 2   author       12671 non-null  object\n",
      " 3   score        12671 non-null  int64 \n",
      " 4   comments     12671 non-null  int64 \n",
      " 5   tag          12671 non-null  object\n",
      " 6   index        12671 non-null  int64 \n",
      " 7   created_utc  12671 non-null  int64 \n",
      " 8   date         12671 non-null  object\n",
      " 9   time         12671 non-null  object\n",
      " 10  title        12671 non-null  object\n",
      " 11  post         12671 non-null  object\n",
      " 12  alltext      12671 non-null  object\n",
      "dtypes: int64(5), object(8)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.852831Z",
     "start_time": "2021-05-06T03:29:20.846561Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nfl    7245\n",
       "epl    5426\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.872097Z",
     "start_time": "2021-05-06T03:29:20.865431Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nfl    0.571778\n",
       "epl    0.428222\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.887842Z",
     "start_time": "2021-05-06T03:29:20.874377Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>target</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>comments</th>\n",
       "      <th>tag</th>\n",
       "      <th>index</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>title</th>\n",
       "      <th>post</th>\n",
       "      <th>alltext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>epl</td>\n",
       "      <td>0</td>\n",
       "      <td>halfbaht</td>\n",
       "      <td>1</td>\n",
       "      <td>329</td>\n",
       "      <td>:brh: Brighton &amp;amp; Hove Albion</td>\n",
       "      <td>10336</td>\n",
       "      <td>1601127114</td>\n",
       "      <td>2020-09-26</td>\n",
       "      <td>06:31:54</td>\n",
       "      <td>How unlucky can one side be?</td>\n",
       "      <td>How unlucky can one side be? Hit the post 5 ti...</td>\n",
       "      <td>How unlucky can one side be? How unlucky can o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>epl</td>\n",
       "      <td>0</td>\n",
       "      <td>TeddyMMR</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>10337</td>\n",
       "      <td>1601127570</td>\n",
       "      <td>2020-09-26</td>\n",
       "      <td>06:39:30</td>\n",
       "      <td>So if we can change decisions after the match ...</td>\n",
       "      <td>The refereeing in this match was horrendous. 2...</td>\n",
       "      <td>So if we can change decisions after the match ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>epl</td>\n",
       "      <td>0</td>\n",
       "      <td>Chrisflev</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Question</td>\n",
       "      <td>10338</td>\n",
       "      <td>1601128425</td>\n",
       "      <td>2020-09-26</td>\n",
       "      <td>06:53:45</td>\n",
       "      <td>Full Player Game Stats</td>\n",
       "      <td>Does anyone know of a site or app that gives y...</td>\n",
       "      <td>Full Player Game Stats Does anyone know of a s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>epl</td>\n",
       "      <td>0</td>\n",
       "      <td>TooSpursy</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>:tot: Tottenham Hotspur</td>\n",
       "      <td>10339</td>\n",
       "      <td>1601129611</td>\n",
       "      <td>2020-09-26</td>\n",
       "      <td>07:13:31</td>\n",
       "      <td>Where to watch PL in the U.S.?</td>\n",
       "      <td>If you live in the U.S. where is the best plac...</td>\n",
       "      <td>Where to watch PL in the U.S.? If you live in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>epl</td>\n",
       "      <td>0</td>\n",
       "      <td>entertainak47</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>Question</td>\n",
       "      <td>10340</td>\n",
       "      <td>1601130904</td>\n",
       "      <td>2020-09-26</td>\n",
       "      <td>07:35:04</td>\n",
       "      <td>Brighton-UTD Penalty:</td>\n",
       "      <td>In the Brighton-UTD game the ref game UTD a pe...</td>\n",
       "      <td>Brighton-UTD Penalty: In the Brighton-UTD game...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>epl</td>\n",
       "      <td>0</td>\n",
       "      <td>gl6ry</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>Question</td>\n",
       "      <td>10341</td>\n",
       "      <td>1601131905</td>\n",
       "      <td>2020-09-26</td>\n",
       "      <td>07:51:45</td>\n",
       "      <td>Which team has the coolest looking jerseys in ...</td>\n",
       "      <td>I’m new to this sport as a fan and trying to d...</td>\n",
       "      <td>Which team has the coolest looking jerseys in ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     subreddit  target         author  score  comments  \\\n",
       "1984       epl       0       halfbaht      1       329   \n",
       "1985       epl       0       TeddyMMR      1        13   \n",
       "1986       epl       0      Chrisflev      1         3   \n",
       "1987       epl       0      TooSpursy      1         8   \n",
       "1988       epl       0  entertainak47      1         8   \n",
       "1989       epl       0          gl6ry      1         6   \n",
       "\n",
       "                                   tag  index  created_utc        date  \\\n",
       "1984  :brh: Brighton &amp; Hove Albion  10336   1601127114  2020-09-26   \n",
       "1985                        Discussion  10337   1601127570  2020-09-26   \n",
       "1986                          Question  10338   1601128425  2020-09-26   \n",
       "1987           :tot: Tottenham Hotspur  10339   1601129611  2020-09-26   \n",
       "1988                          Question  10340   1601130904  2020-09-26   \n",
       "1989                          Question  10341   1601131905  2020-09-26   \n",
       "\n",
       "          time                                              title  \\\n",
       "1984  06:31:54                       How unlucky can one side be?   \n",
       "1985  06:39:30  So if we can change decisions after the match ...   \n",
       "1986  06:53:45                             Full Player Game Stats   \n",
       "1987  07:13:31                     Where to watch PL in the U.S.?   \n",
       "1988  07:35:04                              Brighton-UTD Penalty:   \n",
       "1989  07:51:45  Which team has the coolest looking jerseys in ...   \n",
       "\n",
       "                                                   post  \\\n",
       "1984  How unlucky can one side be? Hit the post 5 ti...   \n",
       "1985  The refereeing in this match was horrendous. 2...   \n",
       "1986  Does anyone know of a site or app that gives y...   \n",
       "1987  If you live in the U.S. where is the best plac...   \n",
       "1988  In the Brighton-UTD game the ref game UTD a pe...   \n",
       "1989  I’m new to this sport as a fan and trying to d...   \n",
       "\n",
       "                                                alltext  \n",
       "1984  How unlucky can one side be? How unlucky can o...  \n",
       "1985  So if we can change decisions after the match ...  \n",
       "1986  Full Player Game Stats Does anyone know of a s...  \n",
       "1987  Where to watch PL in the U.S.? If you live in ...  \n",
       "1988  Brighton-UTD Penalty: In the Brighton-UTD game...  \n",
       "1989  Which team has the coolest looking jerseys in ...  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[1984:1990]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Final Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once columns were cleaned up and the `'alltext'` column was made (from `'title'` and `'selftext'`), I did one more pass over the new column to remove specific strings of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subreddit name strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The names of the subreddits included in titles and body text are likely to be obvious tells for classification, which will be great for our model - helping to ensure high accuracy classification of posts for OverArmor. \n",
    "\n",
    "For EDA, however, removing them might be better, as it will give us a cleaner look at the common vernacular of each community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.957956Z",
     "start_time": "2021-05-06T03:29:20.891277Z"
    }
   },
   "outputs": [],
   "source": [
    "titlecount_alltext_nfl = df[df['alltext'].str.contains('r/nfl')].shape[0] + df[df['alltext'].str.contains('r/NFL')].shape[0]\n",
    "titlecount_alltext_epl = df[df['alltext'].str.contains('r/premierleague')].shape[0] + df[df['alltext'].str.contains('r/PremierLeague')].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.963468Z",
     "start_time": "2021-05-06T03:29:20.959730Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of 'r/nfl' in 'alltext' column: 321\n",
      "Count of 'r/PremierLeague' in 'alltext' column: 41\n"
     ]
    }
   ],
   "source": [
    "print(f\"Count of 'r/nfl' in 'alltext' column: {titlecount_alltext_nfl}\")\n",
    "print(f\"Count of 'r/PremierLeague' in 'alltext' column: {titlecount_alltext_epl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URLs, Punctuation, and Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, I removed all urls and punctuation without exception. This will help me get clean tokens when I get to the point of tokenizing and vectorizing for analysis.\n",
    "\n",
    "I went through many iterations of different code to do it and ultimately got Devin's help and the code inside the 'remove_clutter' function is his."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.968780Z",
     "start_time": "2021-05-06T03:29:20.965705Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_clutter(text):\n",
    "    return re.sub(r'http\\S+', '', text).translate(str.maketrans('', '', string.punctuation)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:20.976086Z",
     "start_time": "2021-05-06T03:29:20.971429Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is the best and I learned a lot from'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = 'https://stackoverflow.com is the best!! and... I learn\\'ed a lot `from https://towardsdatascience.com'\n",
    "\n",
    "remove_clutter(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:21.744862Z",
     "start_time": "2021-05-06T03:29:20.977843Z"
    }
   },
   "outputs": [],
   "source": [
    "for column in ['title', 'post', 'alltext']:\n",
    "    df[column] = df[column].map(lambda x: remove_clutter(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:21.765421Z",
     "start_time": "2021-05-06T03:29:21.748995Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12671 entries, 0 to 12670\n",
      "Data columns (total 13 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   subreddit    12671 non-null  object\n",
      " 1   target       12671 non-null  int64 \n",
      " 2   author       12671 non-null  object\n",
      " 3   score        12671 non-null  int64 \n",
      " 4   comments     12671 non-null  int64 \n",
      " 5   tag          12671 non-null  object\n",
      " 6   index        12671 non-null  int64 \n",
      " 7   created_utc  12671 non-null  int64 \n",
      " 8   date         12671 non-null  object\n",
      " 9   time         12671 non-null  object\n",
      " 10  title        12671 non-null  object\n",
      " 11  post         12671 non-null  object\n",
      " 12  alltext      12671 non-null  object\n",
      "dtypes: int64(5), object(8)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Export for EDA and Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:22.263540Z",
     "start_time": "2021-05-06T03:29:21.768858Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('../data/2_clean/reddit_posts_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:22.275628Z",
     "start_time": "2021-05-06T03:29:22.268228Z"
    }
   },
   "outputs": [],
   "source": [
    "# df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:29:22.280179Z",
     "start_time": "2021-05-06T03:29:22.277450Z"
    }
   },
   "outputs": [],
   "source": [
    "# df[['title', 'post', 'alltext']].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having collected and cleaned our data, I completed my delieverable for OverArmor's first request. \n",
    "\n",
    "It remains to be seen how my models will do, but based on the way the data looks, I expect decent results. I think there are strong enough differences between the language used in these subreddits that the model will be able to do a good job. \n",
    "\n",
    "Team names, city names, unique words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "656px",
    "left": "22px",
    "top": "110px",
    "width": "346px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
