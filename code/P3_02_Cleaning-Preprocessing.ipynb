{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Trebuchet MS; font-size:2em;\">Project 3 | NB2: Cleaning and Preprocessing</span>\n",
    "\n",
    "Riley Robertson | Reddit Classification Project | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I began my process by importing basic libraries and as I cleaned, I returned to add modules as necessary. I also set preferences, assigned variables, imported my data, and set up my main dataframe so I could begin cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:12.320836Z",
     "start_time": "2021-04-30T00:53:10.971714Z"
    }
   },
   "outputs": [],
   "source": [
    "# basics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# custom\n",
    "import utilities.densmore as dns\n",
    "\n",
    "# date and time \n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "# for CVEC test\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:12.325355Z",
     "start_time": "2021-04-30T00:53:12.322980Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:13.830037Z",
     "start_time": "2021-04-30T00:53:12.327585Z"
    }
   },
   "outputs": [],
   "source": [
    "df_nfl = pd.read_csv('../data/raw/raw_nfl_v4.csv', low_memory=False)\n",
    "df_epl = pd.read_csv('../data/raw/raw_epl_v4.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:13.837058Z",
     "start_time": "2021-04-30T00:53:13.832405Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((99661, 13), (99589, 13))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nfl.shape, df_epl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:13.840902Z",
     "start_time": "2021-04-30T00:53:13.838941Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_nfl_full = pd.read_csv('../git_ignore/output/raw_nfl_v4_full.csv', low_memory=False)\n",
    "# df_epl_full = pd.read_csv('../git_ignore/output/raw_epl_v4_full.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:13.847469Z",
     "start_time": "2021-04-30T00:53:13.844609Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_nfl_full.shape, df_epl_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Merging the DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:13.898016Z",
     "start_time": "2021-04-30T00:53:13.851582Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df_epl, df_nfl], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:13.902159Z",
     "start_time": "2021-04-30T00:53:13.900011Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# df_full = pd.concat([df_epl_full, df_nfl_full], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:14.005012Z",
     "start_time": "2021-04-30T00:53:13.903942Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 199250 entries, 0 to 199249\n",
      "Data columns (total 13 columns):\n",
      " #   Column           Non-Null Count   Dtype \n",
      "---  ------           --------------   ----- \n",
      " 0   index            199250 non-null  int64 \n",
      " 1   subreddit        199250 non-null  object\n",
      " 2   created_utc      199250 non-null  int64 \n",
      " 3   author           199250 non-null  object\n",
      " 4   num_comments     199250 non-null  int64 \n",
      " 5   score            199250 non-null  int64 \n",
      " 6   is_self          199250 non-null  bool  \n",
      " 7   link_flair_text  29684 non-null   object\n",
      " 8   title            199250 non-null  object\n",
      " 9   selftext         168312 non-null  object\n",
      " 10  full_link        199250 non-null  object\n",
      " 11  date             199250 non-null  object\n",
      " 12  time             199250 non-null  object\n",
      "dtypes: bool(1), int64(4), object(8)\n",
      "memory usage: 18.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# df.shape\n",
    "# df.head()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Detecting Text Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Using Chardetect terminal command**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:14.009689Z",
     "start_time": "2021-04-30T00:53:14.007457Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ! chardetect ../data/raw/raw_nfl_v4.csv  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Using `with open()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:14.017371Z",
     "start_time": "2021-04-30T00:53:14.013359Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with open('../data/raw/raw_nfl_v4.csv') as f:\n",
    "#     print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Renaming PremierLeague Subreddit to 'epl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:14.039175Z",
     "start_time": "2021-04-30T00:53:14.020679Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nfl              99661\n",
       "PremierLeague    99589\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:14.068431Z",
     "start_time": "2021-04-30T00:53:14.040715Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['subreddit'] = df['subreddit'].map(lambda x: 'epl' if x == 'PremierLeague' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:14.086849Z",
     "start_time": "2021-04-30T00:53:14.069959Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nfl    99661\n",
       "epl    99589\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Column Sorting and Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As I cleaned, I realized that there were some columns that I didn't ultimately need, so I filtered out some of the columns that I initially included in my scraped data and resorted the remaining columns for ease of viewing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:14.107220Z",
     "start_time": "2021-04-30T00:53:14.088621Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = df[['subreddit', 'created_utc', 'link_flair_text', 'author', 'score', 'num_comments', 'index',  'title', 'selftext']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:14.111462Z",
     "start_time": "2021-04-30T00:53:14.108628Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:14.131032Z",
     "start_time": "2021-04-30T00:53:14.113379Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nfl    99661\n",
       "epl    99589\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As shown by the value counts above, I'm starting out with about 100,000 posts for each subreddit. I originally started with fewer, but after I began cleaning, I was quickly running of posts that that had the conditions I wanted. I returned to my Data Collection notebook and increased the number of posts to request from the API so that I'd begin my cleaning with a much greater volume of posts than I would eventually need. That way, I could be more decisive in dropping rows rather than trying to salvage content from posts that had incomplete or low quality information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Nulls only exist in two columns: `link_flair_text` and `selftext`. \n",
    "\n",
    "I knew I had enough data that I could drop all of the posts with empty `selftext` fields, but I didn't want to lose the posts without tags (there are many). So I put 'none' into the `link_flair_text` fields and removed all rows with nulls after that, which left about 80,000 posts per subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:14.185201Z",
     "start_time": "2021-04-30T00:53:14.136281Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 199250 entries, 0 to 199249\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count   Dtype \n",
      "---  ------           --------------   ----- \n",
      " 0   subreddit        199250 non-null  object\n",
      " 1   created_utc      199250 non-null  int64 \n",
      " 2   link_flair_text  29684 non-null   object\n",
      " 3   author           199250 non-null  object\n",
      " 4   score            199250 non-null  int64 \n",
      " 5   num_comments     199250 non-null  int64 \n",
      " 6   index            199250 non-null  int64 \n",
      " 7   title            199250 non-null  object\n",
      " 8   selftext         168312 non-null  object\n",
      "dtypes: int64(4), object(5)\n",
      "memory usage: 13.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:14.200056Z",
     "start_time": "2021-04-30T00:53:14.188623Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['link_flair_text'].fillna('none', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:14.205935Z",
     "start_time": "2021-04-30T00:53:14.203462Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:14.308986Z",
     "start_time": "2021-04-30T00:53:14.207795Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 168312 entries, 0 to 199248\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count   Dtype \n",
      "---  ------           --------------   ----- \n",
      " 0   subreddit        168312 non-null  object\n",
      " 1   created_utc      168312 non-null  int64 \n",
      " 2   link_flair_text  168312 non-null  object\n",
      " 3   author           168312 non-null  object\n",
      " 4   score            168312 non-null  int64 \n",
      " 5   num_comments     168312 non-null  int64 \n",
      " 6   index            168312 non-null  int64 \n",
      " 7   title            168312 non-null  object\n",
      " 8   selftext         168312 non-null  object\n",
      "dtypes: int64(4), object(5)\n",
      "memory usage: 12.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:14.329383Z",
     "start_time": "2021-04-30T00:53:14.312209Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "epl    86351\n",
       "nfl    81961\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Simple Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Dropping duplicates brings down PremierLeague posts to a good range, but the number of NFL posts is still much greater than necessary. As I move forward, I'll work on bringing down the number of NFL posts to at least roughly match that of the PremierLeague posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:14.377683Z",
     "start_time": "2021-04-30T00:53:14.333043Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59730, 9)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates(subset=['title'], inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:14.391501Z",
     "start_time": "2021-04-30T00:53:14.380293Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nfl    53173\n",
       "epl     6557\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:14.613527Z",
     "start_time": "2021-04-30T00:53:14.393259Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56178, 9)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates(subset=['selftext'], inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:14.624110Z",
     "start_time": "2021-04-30T00:53:14.615578Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nfl    49832\n",
       "epl     6346\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Posts with deleted body text (selftext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:14.676971Z",
     "start_time": "2021-04-30T00:53:14.626010Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nfl    49831\n",
       "epl     6324\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(axis=0, \n",
    "        labels=df[df['selftext'].str.startswith('[deleted]')].index, # Submissions with deleted selftext\n",
    "        inplace=True)\n",
    "\n",
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Posts with Markdown tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:14.734210Z",
     "start_time": "2021-04-30T00:53:14.679964Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "markdowns = df[df['selftext'].str.contains('\\|')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:14.756514Z",
     "start_time": "2021-04-30T00:53:14.739352Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nfl    3906\n",
       "epl     201\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markdowns['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:14.771709Z",
     "start_time": "2021-04-30T00:53:14.758366Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.drop(axis=0, labels=markdowns.index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:14.775458Z",
     "start_time": "2021-04-30T00:53:14.773325Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# df['selftext'] = df['selftext'].replace('http\\S+', '', regex=True).replace('www\\S+', '', regex=True)\n",
    "# df['title'] = df['title'].replace('http\\S+', '', regex=True).replace('www\\S+', '', regex=True)\n",
    "\n",
    "# https://stackoverflow.com/questions/11331982/how-to-remove-any-url-within-a-string-in-python/40823105#40823105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:14.781984Z",
     "start_time": "2021-04-30T00:53:14.777210Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I get help from  and I learn a lot reading on '"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_url(text):\n",
    "    import re\n",
    "    return re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', text)\n",
    "\n",
    "# Gwens string didn't work for me:\n",
    "# r'^https?:\\/\\/.*[\\r\\n]*'\n",
    "\n",
    "sentence = 'I get help from https://stackoverflow.com and I learn a lot reading on https://towardsdatascience.com'\n",
    "\n",
    "remove_url(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:16.888678Z",
     "start_time": "2021-04-30T00:53:14.784596Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['selftext'] = df['selftext'].map(lambda x: remove_url(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:16.893420Z",
     "start_time": "2021-04-30T00:53:16.890482Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df['selftext'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Repeated Post Titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Before I got the duplicates removal code working above, I manually went through a list of the most commonly repeating post `'selftext'` and cleared them out with a list and a for-loop. I'm going to remove a bulk of that code since it's no longer necessary, but some of it is still relevant, even after removing duplicates with the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:16.898989Z",
     "start_time": "2021-04-30T00:53:16.895768Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df['selftext'].value_counts()[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:16.903142Z",
     "start_time": "2021-04-30T00:53:16.900866Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "After looking at the value counts for the 'selftext' field, I realized that there were a lot of posts that had the exact same body text. So I used the following code to look at the top 20 most common titles and there were quite a few that were re-used many times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:16.907067Z",
     "start_time": "2021-04-30T00:53:16.904675Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# df['title'].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Readable results from above code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "| Post Title                                         | Count | ┃ | Post Title                          | Count | ┃ | Post Title                                   | Count |\n",
    "|:---------------------------------------------------|:------|:-:|:------------------------------------|:------|:-:|:---------------------------------------------|:------|\n",
    "| Shitpost Saturday                                  | 174   | ┃ | Talko Tuesday                       | 124   | ┃ | r/PremierLeague Midweek Musings              | 13    |\n",
    "| Water Cooler Wednesday                             | 159   | ┃ | r/PremierLeague Daily Discussion    | 71    | ┃ | Weekly /r/PremierLeague Subreddit Suggestion | 11    |\n",
    "| Free Talk Friday                                   | 158   | ┃ | This Week's Top /r/NFL [Highlight]s | 22    | ┃ | Test                                         | 11    |\n",
    "| Sunday Brunch                                      | 157   | ┃ | Weekend Wrap Up                     | 21    | ┃ | Daily Open Discussion Thread                 | 11    |\n",
    "| Thursday Talk Thread... Yes That's The Thread Name | 141   | ┃ | Question                            | 15    | ┃ | Weekly Transfer Discussion Thread            | 8     |\n",
    "| Weekend Wrapup                                     | 130   | ┃ | NFL Power Rankings (Combined)       | 15    | ┃ | test                                         | 7     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "They fell into several categories:\n",
    "1. Open threads meant for discussion of topics of any kind, even if unrelated to the topic of the subreddit.\n",
    "2. Discussion threads in which the topics might be related, but all of the content is in the comments rather than the body of the post\n",
    "3. Posts with code and/or little-to-no useful content\n",
    "4. Commonly used titles by different users to introduce a topic-relevant post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Removing the posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:16.913583Z",
     "start_time": "2021-04-30T00:53:16.909336Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "repeat_titles = [\"Shitpost Saturday\", \"Water Cooler Wednesday\", \"Free Talk Friday\", \"Sunday Brunch\", \n",
    "                 \"Thursday Talk Thread... Yes That's The Thread Name\", \"Weekend Wrapup\", \"Talko Tuesday\",\n",
    "                 \"r/PremierLeague Daily Discussion\", \"This Week's Top /r/NFL [Highlight]s\", \n",
    "                 \"Weekend Wrap Up\", \"NFL Power Rankings (Combined)\", \"r/PremierLeague Midweek Musings\", \n",
    "                 \"Whose Line is it Anyways Wednesday--Offseason Edition\", \n",
    "                 \"Weekly /r/PremierLeague Subreddit Suggestion\", \"Test\", \"Daily Open Discussion Thread\",\n",
    "                 \"Weekly Transfer Discussion Thread\", \"test\", \"Your Weekly /r/nfl Recap\", \n",
    "                 \"NFL Power Rankings (Combined) Week 0\",\n",
    "                 \"Should Ole stay at Manchester United or not? If he got sacked by the board, who will be the best replacement. Comment your thoughts below\"\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.208657Z",
     "start_time": "2021-04-30T00:53:16.915440Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for title in repeat_titles:\n",
    "    title_df = df[df['title'] == title]  \n",
    "    df.drop(axis=0, labels=title_df.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.212410Z",
     "start_time": "2021-04-30T00:53:17.210094Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df['title'].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.223190Z",
     "start_time": "2021-04-30T00:53:17.214641Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nfl    45921\n",
       "epl     6117\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## PremierLeague Poll Posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "I found 91 posts from the PremierLeague subreddit that contained a lot of unnecessary information and formatting was such that vectorizing would be significantly more complicated. I decided to simply remove them for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.232389Z",
     "start_time": "2021-04-30T00:53:17.228243Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('[View Poll](https://www.reddit.com/poll/g437k5)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.270035Z",
     "start_time": "2021-04-30T00:53:17.238397Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "poll_posts = df[df['selftext'].str.startswith('  [View Poll]')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.275961Z",
     "start_time": "2021-04-30T00:53:17.272401Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 9)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poll_posts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.291466Z",
     "start_time": "2021-04-30T00:53:17.278003Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.drop(axis=0, labels=poll_posts.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.301788Z",
     "start_time": "2021-04-30T00:53:17.293244Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nfl    45921\n",
       "epl     6117\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## PremierLeague Match Threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "I found 91 posts from the PremierLeague subreddit that contained a lot of unnecessary information and formatting was such that vectorizing would be significantly more complicated. I decided to simply remove them for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.306237Z",
     "start_time": "2021-04-30T00:53:17.303710Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "match_thread_titles = ('[Match Thread]', \n",
    "                       '[Match thread]', \n",
    "                       '[match Thread]', \n",
    "                       '[match thread]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.336323Z",
     "start_time": "2021-04-30T00:53:17.307891Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    52029\n",
       "True         9\n",
       "Name: title, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'].str.startswith(match_thread_titles).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.362309Z",
     "start_time": "2021-04-30T00:53:17.338259Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    52029\n",
       "True         9\n",
       "Name: title, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'].str.startswith(match_thread_titles).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.384677Z",
     "start_time": "2021-04-30T00:53:17.365101Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "match_threads = df[df['title'].str.startswith(match_thread_titles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.390410Z",
     "start_time": "2021-04-30T00:53:17.386279Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 9)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_threads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.401181Z",
     "start_time": "2021-04-30T00:53:17.392984Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nfl    45921\n",
       "epl     6117\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Removing NFL posts with Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Game Thread, Serious, Look Here!, and others "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.414921Z",
     "start_time": "2021-04-30T00:53:17.402998Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Look Here!                        1805\n",
       "Game Thread                       1061\n",
       "Serious                            748\n",
       "Free Talk                          492\n",
       "Free talk                          437\n",
       "Removed: Rule 2 - Invalid Post     116\n",
       "Post Game Thread                    72\n",
       "Trash Talk                          60\n",
       "Look Here                           51\n",
       "game                                42\n",
       "Name: link_flair_text, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nfl['link_flair_text'].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.432980Z",
     "start_time": "2021-04-30T00:53:17.417021Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nfl_with_tags = df[(df['link_flair_text'] != 'none') & (df['subreddit'] == 'nfl')]\n",
    "df.drop(axis=0, labels=nfl_with_tags.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.444008Z",
     "start_time": "2021-04-30T00:53:17.434574Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "none                       47228\n",
       "Discussion                  1583\n",
       "Question                     919\n",
       "Poll                         564\n",
       ":xpl: Premier League         246\n",
       "News                          79\n",
       ":mun: Manchester United       72\n",
       ":liv: Liverpool               62\n",
       ":ars: Arsenal                 57\n",
       ":che: Chelsea                 53\n",
       "Name: link_flair_text, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['link_flair_text'].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.466704Z",
     "start_time": "2021-04-30T00:53:17.445576Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nfl    45124\n",
       "epl     5553\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(axis=0, labels=df[(df['link_flair_text'] == 'Poll')].index, inplace=True)\n",
    "\n",
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## NFL Posts Filtered by String Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.480312Z",
     "start_time": "2021-04-30T00:53:17.469121Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45124, 9)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered = df[df['subreddit'] == 'nfl']\n",
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.542483Z",
     "start_time": "2021-04-30T00:53:17.482522Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6981, 9)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lengthlimits = df[(df['selftext'].str.len()>500) & \\\n",
    "                     (df['selftext'].str.len()<1200) & \\\n",
    "                     (df['subreddit'] == 'nfl')]\n",
    "df_lengthlimits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.556949Z",
     "start_time": "2021-04-30T00:53:17.544409Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:4163: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "df_filtered.drop(axis=0, labels=df_lengthlimits.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.564205Z",
     "start_time": "2021-04-30T00:53:17.559809Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38143, 9)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.578788Z",
     "start_time": "2021-04-30T00:53:17.565863Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.drop(axis=0, labels=df_filtered.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.587488Z",
     "start_time": "2021-04-30T00:53:17.580513Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nfl    6981\n",
       "epl    5553\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Re-Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "After removing so many rows, the DataFrame's index had gaps in its sequencing, so I decided to reset it to clean it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.592332Z",
     "start_time": "2021-04-30T00:53:17.588970Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.611902Z",
     "start_time": "2021-04-30T00:53:17.596946Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>epl</td>\n",
       "      <td>1619271770</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>CC-33</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>My thoughts and prayers are with Jurgen Klopp ...</td>\n",
       "      <td>Imagine being Jurgen Klopp right now, arguably...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>epl</td>\n",
       "      <td>1619278607</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>Cheerful_Jerry9603</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>Premier League Players who should finish off t...</td>\n",
       "      <td>Chinese Super League is known to be the last p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>epl</td>\n",
       "      <td>1619283368</td>\n",
       "      <td>Question</td>\n",
       "      <td>alphaftw1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>Hypothetical situation, what happens if both c...</td>\n",
       "      <td>So let’s say this season, arsenal win the euro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>epl</td>\n",
       "      <td>1619283692</td>\n",
       "      <td>Question</td>\n",
       "      <td>imjonathvn</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>15</td>\n",
       "      <td>Norwich, Watford, and Bournemouth might all ge...</td>\n",
       "      <td>Norwich and Watford have already been promoted...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit  created_utc link_flair_text              author  score  \\\n",
       "7        epl   1619271770      Discussion               CC-33      1   \n",
       "9        epl   1619278607      Discussion  Cheerful_Jerry9603      1   \n",
       "14       epl   1619283368        Question           alphaftw1      1   \n",
       "15       epl   1619283692        Question          imjonathvn      1   \n",
       "\n",
       "    num_comments  index                                              title  \\\n",
       "7              3      7  My thoughts and prayers are with Jurgen Klopp ...   \n",
       "9              2      9  Premier League Players who should finish off t...   \n",
       "14             8     14  Hypothetical situation, what happens if both c...   \n",
       "15            24     15  Norwich, Watford, and Bournemouth might all ge...   \n",
       "\n",
       "                                             selftext  \n",
       "7   Imagine being Jurgen Klopp right now, arguably...  \n",
       "9   Chinese Super League is known to be the last p...  \n",
       "14  So let’s say this season, arsenal win the euro...  \n",
       "15  Norwich and Watford have already been promoted...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[5:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The 'index' column can serve as a record of each posts original index number in case it's ever needed going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.617821Z",
     "start_time": "2021-04-30T00:53:17.613855Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.632579Z",
     "start_time": "2021-04-30T00:53:17.620943Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>epl</td>\n",
       "      <td>1619271770</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>CC-33</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>My thoughts and prayers are with Jurgen Klopp ...</td>\n",
       "      <td>Imagine being Jurgen Klopp right now, arguably...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>epl</td>\n",
       "      <td>1619278607</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>Cheerful_Jerry9603</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>Premier League Players who should finish off t...</td>\n",
       "      <td>Chinese Super League is known to be the last p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>epl</td>\n",
       "      <td>1619283368</td>\n",
       "      <td>Question</td>\n",
       "      <td>alphaftw1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>Hypothetical situation, what happens if both c...</td>\n",
       "      <td>So let’s say this season, arsenal win the euro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>epl</td>\n",
       "      <td>1619283692</td>\n",
       "      <td>Question</td>\n",
       "      <td>imjonathvn</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>15</td>\n",
       "      <td>Norwich, Watford, and Bournemouth might all ge...</td>\n",
       "      <td>Norwich and Watford have already been promoted...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit  created_utc link_flair_text              author  score  \\\n",
       "5       epl   1619271770      Discussion               CC-33      1   \n",
       "6       epl   1619278607      Discussion  Cheerful_Jerry9603      1   \n",
       "7       epl   1619283368        Question           alphaftw1      1   \n",
       "8       epl   1619283692        Question          imjonathvn      1   \n",
       "\n",
       "   num_comments  index                                              title  \\\n",
       "5             3      7  My thoughts and prayers are with Jurgen Klopp ...   \n",
       "6             2      9  Premier League Players who should finish off t...   \n",
       "7             8     14  Hypothetical situation, what happens if both c...   \n",
       "8            24     15  Norwich, Watford, and Bournemouth might all ge...   \n",
       "\n",
       "                                            selftext  \n",
       "5  Imagine being Jurgen Klopp right now, arguably...  \n",
       "6  Chinese Super League is known to be the last p...  \n",
       "7  So let’s say this season, arsenal win the euro...  \n",
       "8  Norwich and Watford have already been promoted...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[5:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Column Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.636786Z",
     "start_time": "2021-04-30T00:53:17.634480Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Converting `'created_utc'` to `'datetime'`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.657437Z",
     "start_time": "2021-04-30T00:53:17.639045Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['datetime'] = df['created_utc'].map(lambda x: dt.datetime.fromtimestamp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Renaming `'selftext'` to `'post'`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.667315Z",
     "start_time": "2021-04-30T00:53:17.659798Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['post'] = df['selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.682562Z",
     "start_time": "2021-04-30T00:53:17.669355Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.drop(columns='selftext', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Merging `'title'` and `'post'` into an `'alltext'` column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.699956Z",
     "start_time": "2021-04-30T00:53:17.684300Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['alltext'] = df['title'] + ' ' + df['post']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.706001Z",
     "start_time": "2021-04-30T00:53:17.702592Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.718032Z",
     "start_time": "2021-04-30T00:53:17.707762Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>post</th>\n",
       "      <th>alltext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>Which player(s) currently at your club, if any, have the potential to go down as club legends?</td>\n",
       "      <td>When I say legends, I mean the likes of Moore for West Ham, Henry for Arsenal etc. and not cult heros (like Michu for Swansea). For us (the Hammers) Noble is basically a legend already, and I could definitely see Rice joining him dependent on how long he stays with us/if he achieves anything with us.</td>\n",
       "      <td>Which player(s) currently at your club, if any, have the potential to go down as club legends? When I say legends, I mean the likes of Moore for West Ham, Henry for Arsenal etc. and not cult heros (like Michu for Swansea). For us (the Hammers) Noble is basically a legend already, and I could definitely see Rice joining him dependent on how long he stays with us/if he achieves anything with us.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              title  \\\n",
       "343  Which player(s) currently at your club, if any, have the potential to go down as club legends?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                              post  \\\n",
       "343  When I say legends, I mean the likes of Moore for West Ham, Henry for Arsenal etc. and not cult heros (like Michu for Swansea). For us (the Hammers) Noble is basically a legend already, and I could definitely see Rice joining him dependent on how long he stays with us/if he achieves anything with us.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                          alltext  \n",
       "343  Which player(s) currently at your club, if any, have the potential to go down as club legends? When I say legends, I mean the likes of Moore for West Ham, Henry for Arsenal etc. and not cult heros (like Michu for Swansea). For us (the Hammers) Noble is basically a legend already, and I could definitely see Rice joining him dependent on how long he stays with us/if he achieves anything with us.  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checks\n",
    "pd.DataFrame(df.iloc[343]).T[['title', 'post', 'alltext']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.728960Z",
     "start_time": "2021-04-30T00:53:17.720179Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(395, 396)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pd.DataFrame(df.iloc[343]).T['title'][343]) + len(pd.DataFrame(df.iloc[343]).T['post'][343]), \\\n",
    "len(pd.DataFrame(df.iloc[343]).T['alltext'][343])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Renaming `'num_comments'` to `'comments'`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Shortening column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.741850Z",
     "start_time": "2021-04-30T00:53:17.732396Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['comments'] = df['num_comments']\n",
    "\n",
    "df.drop(columns='num_comments', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Renaming `'link_flair_text'` to `'tag'`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.753658Z",
     "start_time": "2021-04-30T00:53:17.743537Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['tag'] = df['link_flair_text']\n",
    "\n",
    "df.drop(columns='link_flair_text', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Creating `'target'` column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.762050Z",
     "start_time": "2021-04-30T00:53:17.755507Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['target'] = df['subreddit'].map(lambda x: 0 if x == 'nfl' else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Re-ordering columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.768995Z",
     "start_time": "2021-04-30T00:53:17.764000Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = df[['subreddit', 'target', 'author', 'score', 'comments', 'tag', 'index', 'alltext']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.781365Z",
     "start_time": "2021-04-30T00:53:17.771474Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12534 entries, 0 to 12533\n",
      "Data columns (total 8 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   subreddit  12534 non-null  object\n",
      " 1   target     12534 non-null  int64 \n",
      " 2   author     12534 non-null  object\n",
      " 3   score      12534 non-null  int64 \n",
      " 4   comments   12534 non-null  int64 \n",
      " 5   tag        12534 non-null  object\n",
      " 6   index      12534 non-null  int64 \n",
      " 7   alltext    12534 non-null  object\n",
      "dtypes: int64(4), object(4)\n",
      "memory usage: 783.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.790296Z",
     "start_time": "2021-04-30T00:53:17.783672Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nfl    6981\n",
       "epl    5553\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.804322Z",
     "start_time": "2021-04-30T00:53:17.792571Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nfl    0.556965\n",
       "epl    0.443035\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:17.822293Z",
     "start_time": "2021-04-30T00:53:17.806687Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>target</th>\n",
       "      <th>author</th>\n",
       "      <th>score</th>\n",
       "      <th>comments</th>\n",
       "      <th>tag</th>\n",
       "      <th>index</th>\n",
       "      <th>alltext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>epl</td>\n",
       "      <td>1</td>\n",
       "      <td>shahs210</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>News</td>\n",
       "      <td>10237</td>\n",
       "      <td>Free last man standing competition. Join if in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>epl</td>\n",
       "      <td>1</td>\n",
       "      <td>imR0N</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>:liv: Liverpool</td>\n",
       "      <td>10238</td>\n",
       "      <td>Reasons behind signing Diogo Jota for 40 milli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>epl</td>\n",
       "      <td>1</td>\n",
       "      <td>___ratsalad</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>:xpl: Premier League</td>\n",
       "      <td>10239</td>\n",
       "      <td>[New Series] The Football Book Club podcast He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>epl</td>\n",
       "      <td>1</td>\n",
       "      <td>SmithBurger</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Question</td>\n",
       "      <td>10240</td>\n",
       "      <td>When are replays available on peacock? The pea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>epl</td>\n",
       "      <td>1</td>\n",
       "      <td>trashcan_paradise</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>none</td>\n",
       "      <td>10241</td>\n",
       "      <td>A suggestion for Americans looking for an EPL ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>epl</td>\n",
       "      <td>1</td>\n",
       "      <td>zorfog</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>:xpl: Premier League</td>\n",
       "      <td>10242</td>\n",
       "      <td>Just a casual reminder to #BoycottPeacock This...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     subreddit  target             author  score  comments  \\\n",
       "1974       epl       1           shahs210      1         0   \n",
       "1975       epl       1              imR0N      1         5   \n",
       "1976       epl       1        ___ratsalad      1         0   \n",
       "1977       epl       1        SmithBurger      1         1   \n",
       "1978       epl       1  trashcan_paradise      1        33   \n",
       "1979       epl       1             zorfog      1        24   \n",
       "\n",
       "                       tag  index  \\\n",
       "1974                  News  10237   \n",
       "1975       :liv: Liverpool  10238   \n",
       "1976  :xpl: Premier League  10239   \n",
       "1977              Question  10240   \n",
       "1978                  none  10241   \n",
       "1979  :xpl: Premier League  10242   \n",
       "\n",
       "                                                alltext  \n",
       "1974  Free last man standing competition. Join if in...  \n",
       "1975  Reasons behind signing Diogo Jota for 40 milli...  \n",
       "1976  [New Series] The Football Book Club podcast He...  \n",
       "1977  When are replays available on peacock? The pea...  \n",
       "1978  A suggestion for Americans looking for an EPL ...  \n",
       "1979  Just a casual reminder to #BoycottPeacock This...  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.reset_option('display.max_colwidth')\n",
    "df[1974:1980]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:18.067027Z",
     "start_time": "2021-04-30T00:53:17.824710Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Subreddit name strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The names of the subreddits included in titles and body text are likely to be obvious tells for classification, which will be great for our model - helping to ensure high accuracy classification of posts for OverArmor. \n",
    "\n",
    "For EDA, however, removing them might be better, as it will give us a cleaner look at the common vernacular of each community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:18.130446Z",
     "start_time": "2021-04-30T00:53:18.068570Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "titlecount_alltext_nfl = df[df['alltext'].str.contains('r/nfl')].shape[0] + df[df['alltext'].str.contains('r/NFL')].shape[0]\n",
    "titlecount_alltext_epl = df[df['alltext'].str.contains('r/premierleague')].shape[0] + df[df['alltext'].str.contains('r/PremierLeague')].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:18.134614Z",
     "start_time": "2021-04-30T00:53:18.132148Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of 'r/nfl' in 'alltext' column: 211\n",
      "Count of 'r/PremierLeague' in 'alltext' column: 29\n"
     ]
    }
   ],
   "source": [
    "print(f\"Count of 'r/nfl' in 'alltext' column: {titlecount_alltext_nfl}\")\n",
    "print(f\"Count of 'r/PremierLeague' in 'alltext' column: {titlecount_alltext_epl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:18.138218Z",
     "start_time": "2021-04-30T00:53:18.136366Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# we can remove these for EDA later\n",
    "# nfl_titles, epl_titles, blanks = ('r/nfl', 'r/NFL'), ('r/premierleague', 'r/PremierLeague'), ('','')\n",
    "# df.replace(nfl_titles, blanks, inplace=True)\n",
    "# df.replace(epl_titles, blanks, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:18.160306Z",
     "start_time": "2021-04-30T00:53:18.156252Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# df.replace(nfl_titles, blanks, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:18.166570Z",
     "start_time": "2021-04-30T00:53:18.162677Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# df.replace(epl_titles, blanks, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:18.175577Z",
     "start_time": "2021-04-30T00:53:18.169920Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# df['alltext'][100:120]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For simplicity, I'm replacing the code for various symbols with spaces. This will help me get at clean tokens when I get to the point of tokenizing and vectorizing for analysis.  \n",
    "\n",
    "My first attempts to replace the shortcodes failed, so I went online and found a good solution using simple RegEx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:18.179475Z",
     "start_time": "2021-04-30T00:53:18.177228Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/44227748/removing-newlines-from-messy-strings-in-pandas-dataframe-cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:20.541617Z",
     "start_time": "2021-04-30T00:53:18.182998Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# \\r (return)\n",
    "df.replace('\\r',' ', regex=True, inplace=True) \n",
    "\n",
    "# \\n (line break)\n",
    "df.replace('\\n',' ', regex=True, inplace=True)   \n",
    "\n",
    "# \\t (tab)\n",
    "df.replace('\\t',' ', regex=True, inplace=True)   \n",
    "\n",
    "# &amp; (&)\n",
    "df.replace('&amp;',' ', regex=True, inplace=True)   \n",
    "\n",
    "# &nbsp; (space)\n",
    "df.replace('&nbsp;',' ', regex=True, inplace=True)  \n",
    "\n",
    "# nbsp; (space, chained to other code)\n",
    "df.replace('nbsp;',' ', regex=True, inplace=True)    \n",
    "\n",
    "# # &gt; and &lt; (> and <)\n",
    "df.replace('&lt;','', regex=True, inplace=True)   \n",
    "df.replace('&gt;','', regex=True, inplace=True) \n",
    "\n",
    "# #x200b    # not working even after many iterations of the code \n",
    "df.replace('x200B','', inplace=True) \n",
    "\n",
    "# ' (apostrophe)\n",
    "df.replace(\"'*\", '', regex=True, inplace=True)\n",
    "\n",
    "# **\n",
    "df.replace('[\\*\\*]', '', regex=True, inplace=True) \n",
    "\n",
    "# 49ers # also not working. very confusing because it seems really simple.\n",
    "df.replace('49ers', 'fortyniners', inplace=True) \n",
    "\n",
    "# ‚Äì \n",
    "df.replace('‚Äì', '', inplace=True)\n",
    "\n",
    "# [ (left bracket)\n",
    "df.replace('\\[', '', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "I also tried to loop through a list to make this more efficient, but had trouble getting it to work, possibly because the r-strings didn't come through the list correctly or something?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:20.545371Z",
     "start_time": "2021-04-30T00:53:20.543113Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# symbols = ['&amp;', '&nbsp;', 'nbsp;', '&lt;', '&gt;', '\\*\\*']\n",
    "\n",
    "# for symbol in symbols:\n",
    "#     df.replace(symbol, ' ', regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:20.552768Z",
     "start_time": "2021-04-30T00:53:20.547884Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df['selftext'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There is still cleaning to be done with regard to symbols and symbol code, but I'll circle back to this once I've done more row removal, as I might end up removing the rows that have the problematic text strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:20.560167Z",
     "start_time": "2021-04-30T00:53:20.554487Z"
    }
   },
   "outputs": [],
   "source": [
    "df_m = df[['subreddit', 'target', 'score', 'comments', 'tag', 'alltext']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GetDummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:20.568018Z",
     "start_time": "2021-04-30T00:53:20.562976Z"
    }
   },
   "outputs": [],
   "source": [
    "df_m_dums = pd.get_dummies(df_m['tag'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:20.575800Z",
     "start_time": "2021-04-30T00:53:20.570606Z"
    }
   },
   "outputs": [],
   "source": [
    "df_m = pd.concat([df_m, df_m_dums], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:20.584062Z",
     "start_time": "2021-04-30T00:53:20.577510Z"
    }
   },
   "outputs": [],
   "source": [
    "df_m.drop(columns=['tag'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:20.602858Z",
     "start_time": "2021-04-30T00:53:20.585771Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>target</th>\n",
       "      <th>score</th>\n",
       "      <th>comments</th>\n",
       "      <th>alltext</th>\n",
       "      <th>:ava: Aston Villa</th>\n",
       "      <th>:brh: Brighton   Hove Albion</th>\n",
       "      <th>:bur: Burnley</th>\n",
       "      <th>:che: Chelsea</th>\n",
       "      <th>:cry: Crystal Palace</th>\n",
       "      <th>...</th>\n",
       "      <th>Question</th>\n",
       "      <th>Rumor</th>\n",
       "      <th>Tottenham Hotspur</th>\n",
       "      <th>Transfer News</th>\n",
       "      <th>Transfer Rumor</th>\n",
       "      <th>Watford FC</th>\n",
       "      <th>West Ham United</th>\n",
       "      <th>Who to Root for</th>\n",
       "      <th>Wolverhampton Wanderers</th>\n",
       "      <th>none</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>epl</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>164</td>\n",
       "      <td>One day I hope Mourinho will go somewhere wher...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>epl</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Whats the best place to watch Premier league a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>epl</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Forget obsessing about the ESL... the REAL pro...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>epl</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>Tough day to be a Liverpool supporter. 22 shot...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>epl</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Justice served. Fuck VAR I can’t believe that ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit  target  score  comments  \\\n",
       "0       epl       1      1       164   \n",
       "1       epl       1      1         3   \n",
       "2       epl       1      1         1   \n",
       "3       epl       1      1        16   \n",
       "4       epl       1      1         2   \n",
       "\n",
       "                                             alltext  :ava: Aston Villa  \\\n",
       "0  One day I hope Mourinho will go somewhere wher...                  0   \n",
       "1  Whats the best place to watch Premier league a...                  0   \n",
       "2  Forget obsessing about the ESL... the REAL pro...                  0   \n",
       "3  Tough day to be a Liverpool supporter. 22 shot...                  0   \n",
       "4  Justice served. Fuck VAR I can’t believe that ...                  0   \n",
       "\n",
       "   :brh: Brighton   Hove Albion  :bur: Burnley  :che: Chelsea  \\\n",
       "0                             0              0              0   \n",
       "1                             0              0              0   \n",
       "2                             0              0              0   \n",
       "3                             0              0              0   \n",
       "4                             0              0              0   \n",
       "\n",
       "   :cry: Crystal Palace  ...  Question  Rumor  Tottenham Hotspur  \\\n",
       "0                     0  ...         0      0                  0   \n",
       "1                     0  ...         1      0                  0   \n",
       "2                     0  ...         0      0                  0   \n",
       "3                     0  ...         0      0                  0   \n",
       "4                     0  ...         0      0                  0   \n",
       "\n",
       "   Transfer News  Transfer Rumor  Watford FC  West Ham United  \\\n",
       "0              0               0           0                0   \n",
       "1              0               0           0                0   \n",
       "2              0               0           0                0   \n",
       "3              0               0           0                0   \n",
       "4              0               0           0                0   \n",
       "\n",
       "   Who to Root for  Wolverhampton Wanderers  none  \n",
       "0                0                        0     0  \n",
       "1                0                        0     0  \n",
       "2                0                        0     0  \n",
       "3                0                        0     0  \n",
       "4                0                        0     0  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_m.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T23:54:23.150224Z",
     "start_time": "2021-04-29T23:54:23.147046Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# df_m.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T23:54:23.157441Z",
     "start_time": "2021-04-29T23:54:23.153002Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "X = df_m.drop(columns=['subreddit', 'target'])\n",
    "y = df_m['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T23:54:23.176059Z",
     "start_time": "2021-04-29T23:54:23.159775Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=74, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:13:11.396189Z",
     "start_time": "2021-04-30T00:13:11.393514Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# X_train.head()\n",
    "# X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T23:54:23.187471Z",
     "start_time": "2021-04-29T23:54:23.180514Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# add_stop_words = []\n",
    "stpwds = text.ENGLISH_STOP_WORDS #.union(add_stop_words) # < uncomment to add custom stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-29T23:54:23.195634Z",
     "start_time": "2021-04-29T23:54:23.191159Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# instantiate the transformer\n",
    "tvec = TfidfVectorizer(stop_words=stpwds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:15:00.515256Z",
     "start_time": "2021-04-30T00:14:59.366205Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "X_train_tvec = tvec.fit_transform(X_train['alltext'])\n",
    "X_test_tvec = tvec.transform(X_test['alltext'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:15:10.086563Z",
     "start_time": "2021-04-30T00:15:10.081806Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_tvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:15:15.064250Z",
     "start_time": "2021-04-30T00:15:14.080509Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>00002</th>\n",
       "      <th>0009</th>\n",
       "      <th>000k</th>\n",
       "      <th>000s</th>\n",
       "      <th>000yd</th>\n",
       "      <th>001</th>\n",
       "      <th>003</th>\n",
       "      <th>005</th>\n",
       "      <th>...</th>\n",
       "      <th>좋은</th>\n",
       "      <th>중요하다</th>\n",
       "      <th>지고</th>\n",
       "      <th>하고</th>\n",
       "      <th>하는</th>\n",
       "      <th>하는데</th>\n",
       "      <th>해도</th>\n",
       "      <th>해오고</th>\n",
       "      <th>했는데</th>\n",
       "      <th>희생과</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29158 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00  000  00002  0009  000k  000s  000yd  001  003  005  ...   좋은  중요하다  \\\n",
       "0  0.0  0.0    0.0   0.0   0.0   0.0    0.0  0.0  0.0  0.0  ...  0.0   0.0   \n",
       "1  0.0  0.0    0.0   0.0   0.0   0.0    0.0  0.0  0.0  0.0  ...  0.0   0.0   \n",
       "2  0.0  0.0    0.0   0.0   0.0   0.0    0.0  0.0  0.0  0.0  ...  0.0   0.0   \n",
       "3  0.0  0.0    0.0   0.0   0.0   0.0    0.0  0.0  0.0  0.0  ...  0.0   0.0   \n",
       "4  0.0  0.0    0.0   0.0   0.0   0.0    0.0  0.0  0.0  0.0  ...  0.0   0.0   \n",
       "\n",
       "    지고   하고   하는  하는데   해도  해오고  했는데  희생과  \n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 29158 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert training data to dataframe\n",
    "X_train_df = pd.DataFrame(X_train_tvec.todense(), columns=tvec.get_feature_names())\n",
    "X_train_df = pd.DataFrame(X_train_tvec.todense(), columns=tvec.get_feature_names())\n",
    "X_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:39:41.635773Z",
     "start_time": "2021-04-30T00:39:41.228035Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>00002</th>\n",
       "      <th>0009</th>\n",
       "      <th>000k</th>\n",
       "      <th>000s</th>\n",
       "      <th>000yd</th>\n",
       "      <th>001</th>\n",
       "      <th>003</th>\n",
       "      <th>005</th>\n",
       "      <th>...</th>\n",
       "      <th>좋은</th>\n",
       "      <th>중요하다</th>\n",
       "      <th>지고</th>\n",
       "      <th>하고</th>\n",
       "      <th>하는</th>\n",
       "      <th>하는데</th>\n",
       "      <th>해도</th>\n",
       "      <th>해오고</th>\n",
       "      <th>했는데</th>\n",
       "      <th>희생과</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29158 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00  000  00002  0009  000k  000s  000yd  001  003  005  ...   좋은  중요하다  \\\n",
       "0  0.0  0.0    0.0   0.0   0.0   0.0    0.0  0.0  0.0  0.0  ...  0.0   0.0   \n",
       "1  0.0  0.0    0.0   0.0   0.0   0.0    0.0  0.0  0.0  0.0  ...  0.0   0.0   \n",
       "2  0.0  0.0    0.0   0.0   0.0   0.0    0.0  0.0  0.0  0.0  ...  0.0   0.0   \n",
       "3  0.0  0.0    0.0   0.0   0.0   0.0    0.0  0.0  0.0  0.0  ...  0.0   0.0   \n",
       "4  0.0  0.0    0.0   0.0   0.0   0.0    0.0  0.0  0.0  0.0  ...  0.0   0.0   \n",
       "\n",
       "    지고   하고   하는  하는데   해도  해오고  했는데  희생과  \n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 29158 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_df = pd.DataFrame(X_test_tvec.todense(), columns=tvec.get_feature_names())\n",
    "X_test_df = pd.DataFrame(X_test_tvec.todense(), columns=tvec.get_feature_names())\n",
    "X_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:41:25.246794Z",
     "start_time": "2021-04-30T00:41:25.203056Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "X_train_df.reset_index(drop=True, inplace=True)\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test_df.reset_index(drop=True, inplace = True)\n",
    "X_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:41:36.219066Z",
     "start_time": "2021-04-30T00:41:31.815685Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>comments</th>\n",
       "      <th>alltext</th>\n",
       "      <th>:ava: Aston Villa</th>\n",
       "      <th>:brh: Brighton   Hove Albion</th>\n",
       "      <th>:bur: Burnley</th>\n",
       "      <th>:che: Chelsea</th>\n",
       "      <th>:cry: Crystal Palace</th>\n",
       "      <th>:eve: Everton</th>\n",
       "      <th>:ful: Fulham</th>\n",
       "      <th>...</th>\n",
       "      <th>좋은</th>\n",
       "      <th>중요하다</th>\n",
       "      <th>지고</th>\n",
       "      <th>하고</th>\n",
       "      <th>하는</th>\n",
       "      <th>하는데</th>\n",
       "      <th>해도</th>\n",
       "      <th>해오고</th>\n",
       "      <th>했는데</th>\n",
       "      <th>희생과</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>If your club can choose   pick for free any pl...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>105</td>\n",
       "      <td>238</td>\n",
       "      <td>Conversely, every season it seems like one or ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>258</td>\n",
       "      <td>81</td>\n",
       "      <td>Nick Foles could make Playoff History tomorrow...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>59</td>\n",
       "      <td>Patriots had only two pro bowlers but all thei...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>Where will Sheffield United finish this year? ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29213 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   score  comments                                            alltext  \\\n",
       "0      1        40  If your club can choose   pick for free any pl...   \n",
       "1    105       238  Conversely, every season it seems like one or ...   \n",
       "2    258        81  Nick Foles could make Playoff History tomorrow...   \n",
       "3     32        59  Patriots had only two pro bowlers but all thei...   \n",
       "4      1        17  Where will Sheffield United finish this year? ...   \n",
       "\n",
       "   :ava: Aston Villa  :brh: Brighton   Hove Albion  :bur: Burnley  \\\n",
       "0                  0                             0              0   \n",
       "1                  0                             0              0   \n",
       "2                  0                             0              0   \n",
       "3                  0                             0              0   \n",
       "4                  0                             0              0   \n",
       "\n",
       "   :che: Chelsea  :cry: Crystal Palace  :eve: Everton  :ful: Fulham  ...   좋은  \\\n",
       "0              0                     0              0             0  ...  0.0   \n",
       "1              0                     0              0             0  ...  0.0   \n",
       "2              0                     0              0             0  ...  0.0   \n",
       "3              0                     0              0             0  ...  0.0   \n",
       "4              0                     0              0             0  ...  0.0   \n",
       "\n",
       "   중요하다   지고   하고   하는  하는데   해도  해오고  했는데  희생과  \n",
       "0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 29213 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_all = pd.concat([X_train, X_train_df],axis = 1)\n",
    "X_train_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:43:51.071887Z",
     "start_time": "2021-04-30T00:43:49.581024Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>comments</th>\n",
       "      <th>alltext</th>\n",
       "      <th>:ava: Aston Villa</th>\n",
       "      <th>:brh: Brighton   Hove Albion</th>\n",
       "      <th>:bur: Burnley</th>\n",
       "      <th>:che: Chelsea</th>\n",
       "      <th>:cry: Crystal Palace</th>\n",
       "      <th>:eve: Everton</th>\n",
       "      <th>:ful: Fulham</th>\n",
       "      <th>...</th>\n",
       "      <th>좋은</th>\n",
       "      <th>중요하다</th>\n",
       "      <th>지고</th>\n",
       "      <th>하고</th>\n",
       "      <th>하는</th>\n",
       "      <th>하는데</th>\n",
       "      <th>해도</th>\n",
       "      <th>해오고</th>\n",
       "      <th>했는데</th>\n",
       "      <th>희생과</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>MANCHESTER UNITED LACK OF SUMMER BUSINESS COST...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>Rivalry help for an American Manchester United...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>Which of the following young players have been...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Premier League or A-League? Which do you prefer?</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>Have defenses just absolutely been getting tor...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29213 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   score  comments                                            alltext  \\\n",
       "0      0         0  MANCHESTER UNITED LACK OF SUMMER BUSINESS COST...   \n",
       "1      1        32  Rivalry help for an American Manchester United...   \n",
       "2      1         6  Which of the following young players have been...   \n",
       "3      1         0   Premier League or A-League? Which do you prefer?   \n",
       "4      1        34  Have defenses just absolutely been getting tor...   \n",
       "\n",
       "   :ava: Aston Villa  :brh: Brighton   Hove Albion  :bur: Burnley  \\\n",
       "0                  0                             0              0   \n",
       "1                  0                             0              0   \n",
       "2                  0                             0              0   \n",
       "3                  0                             0              0   \n",
       "4                  0                             0              0   \n",
       "\n",
       "   :che: Chelsea  :cry: Crystal Palace  :eve: Everton  :ful: Fulham  ...   좋은  \\\n",
       "0              0                     0              0             0  ...  0.0   \n",
       "1              0                     0              0             0  ...  0.0   \n",
       "2              0                     0              0             0  ...  0.0   \n",
       "3              0                     0              0             0  ...  0.0   \n",
       "4              0                     0              0             0  ...  0.0   \n",
       "\n",
       "   중요하다   지고   하고   하는  하는데   해도  해오고  했는데  희생과  \n",
       "0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 29213 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_all = pd.concat([X_test, X_test_df],axis = 1)\n",
    "X_test_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:20.869185Z",
     "start_time": "2021-04-30T00:53:20.604607Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('../data/reddit_posts_clean_eda.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:21.257886Z",
     "start_time": "2021-04-30T00:53:20.871247Z"
    }
   },
   "outputs": [],
   "source": [
    "df_m.to_csv('../data/reddit_posts_clean_modeling.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:21.269836Z",
     "start_time": "2021-04-30T00:53:21.263171Z"
    }
   },
   "outputs": [],
   "source": [
    "# df[df['post'].str.len()>2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:21.276038Z",
     "start_time": "2021-04-30T00:53:21.272316Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T00:53:21.287130Z",
     "start_time": "2021-04-30T00:53:21.278061Z"
    }
   },
   "outputs": [],
   "source": [
    "# df[df['subreddit'] == 'nfl'].sort_values(by='score', ascending=False).head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "656px",
    "left": "22px",
    "top": "110px",
    "width": "346px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
